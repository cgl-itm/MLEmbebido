# Knowledge Distillation

## Articles
* [A Comprehensive Review of Knowledge Distillation in Computer Vision
](https://arxiv.org/abs/2404.00936)

## Blogs
* [Build Powerful Lightweight Models Using Knowledge Distillation](https://towardsdatascience.com/build-powerful-lightweight-models-using-knowledge-distillation-618f69b569d9)
* [From TensorFlow to TFLite](https://medium.com/@zone24x7_inc/from-tensorflow-to-tflite-how-model-conversion-is-done-and-how-it-affects-neural-network-structure-1d01086083e0)
* [Seungki Kim](https://medium.com/@poperson1205)

## YouTube
* [Knowledge Distillation in Deep Neural Network](https://www.youtube.com/watch?v=83FFn7GqLu0)
* [Better not Bigger: Distilling LLMs into Specialized Models](https://www.youtube.com/watch?v=TIqf4LMNCjU)
* 

## Repositories
* [Knowledge-Distillation-in-Keras](https://github.com/sayakpaul/Knowledge-Distillation-in-Keras/tree/master)
* [Repositorio completo](https://github.com/dkozlov/awesome-knowledge-distillation)
* [TorchDistill](https://github.com/yoshitomo-matsubara/torchdistill)
